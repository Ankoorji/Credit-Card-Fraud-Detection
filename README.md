# Credit-Card-Fraud-Detection

Credit Cards are widely used throughout the world due to the fast & easy processes in applying for one and a win-win situation for both the credit card customers (as it increases the purchasing power) as well as the bank (which generates income from numerous avenues of maintenance fees, late interest charges etc.) However, with increase in use of credit card, the frauds have also multi-fold.
In the dataset (from Kaggle) provided to us, there are around 2,84,807 transactions (in span of 2 days) and out of which fraud is detected for 492 transactions which is even < 1% in fact almost just 0.2%. Also, the data is already masked by performing the PCA which has given 31 variables in total including the target variable. While almost all variables are denoted as V1, V2…V28, there are 2 independent variables which is having semantic significance viz. “Amount” and “Time”. ‘Time’ tells about the time spent between different transactions, using this it can be identified how frequent transactions have been made. ‘Amount’ tells about the transaction amount. The independent variable or class variable is denoted with label “Class’ which contains 1 and 0 an indicator of whether the transaction is fraud or not.

To approach this problem statement, first of all EDA can be performed to see the distribution and relationship between the dependent and independent variables. Some of the variables such as V6, V8 are showing skewness and it has to be handled as it has a negative impact on building a good model. To deal with the skewness/outliers, some of the techniques that can be employed are Power transformation, Box-Cox transformation etc. Need to apply these techniques to check which one works best to handle the skewness effectively. Also, due to high imbalance in dataset, class balancing techniques such as uniform sampling, random sampling, SMOTE, ADASYN etc. will be testes. Since, data is very costly always; techniques which are not adding new information or not adding value to the data will not be used. It means that uniform sampling and random sampling might not be a real contender to assist in class balancing together with adding information to the dataset.

Post dealing with class imbalance, dataset is divided into train-test split and since the entire dataset is numerical and none of it is categorical, no other method is required to be deployed post performing the train-test split. Thereafter, predictive modeling techniques for classification problems need to be tested in the order of use-> Logistic regression, Decision Trees, KNN, SVM, Random Forest. The model which performs the best will be picked up for the analysis. Advance techniques such as XGBoost can also be tried if the other 5 techniques don’t give satisfactory results.

In order to check the performance of the models, ROC_AUC score is used together with the confusion matrix. The AUC curve is in general the best predictor of any classification problem overall. Along with these parameters derived from confusion matrix such as precision and recall are also used to check the model performance.

Hence, final model will be the one which best explains the variables affecting the fraudulent transactions along with the highest AUC score.
